{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Project_Code_TechTitans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "6i69UGjpM8B0",
        "outputId": "5d41c887-3f96-47e4-a107-291c80aa0bf4"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/input/sentiment-analysis-of-tweets/test_samples.txt\n",
            "/kaggle/input/sentiment-analysis-of-tweets/train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "HqCgHpNVM8B7"
      },
      "source": [
        "#importing all the libraries\n",
        "\n",
        "import sys , os , re , csv , codecs , numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense,Input,LSTM,Embedding,Dropout,Activation,BatchNormalization\n",
        "from keras.layers import Bidirectional,GlobalMaxPool1D,GlobalAvgPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints , optimizers, layers\n",
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzu-6ASRM8B-"
      },
      "source": [
        "#getting the train data\n",
        "train = pd.read_csv('/kaggle/input/sentiment-analysis-of-tweets/train.txt')\n",
        "#loading the test data\n",
        "test = pd.read_csv('/kaggle/input/sentiment-analysis-of-tweets/test_samples.txt')\n",
        "#diplay first 5 rows of train\n",
        "train.head()\n",
        "#one hot encoding the labels\n",
        "df = pd.concat([train,pd.get_dummies(train['sentiment'])],axis=1)\n",
        "#df.head()\n",
        "train_data = df['tweet_text']\n",
        "#train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7nRKIl5M8CC",
        "outputId": "3fc3e3f4-700a-4d77-f497-293f9fcb8727"
      },
      "source": [
        "test_data = test['tweet_text']\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    @jjuueellzz down in the Atlantic city, ventnor...\n",
              "1    Musical awareness: Great Big Beautiful Tomorro...\n",
              "2    On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...\n",
              "3    Kapan sih lo ngebuktiin,jan ngomong doang Susa...\n",
              "4    Excuse the connectivity of this live stream, f...\n",
              "Name: tweet_text, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VXqmkz_M8CF",
        "outputId": "29319bdc-6b6b-410e-820a-3cfb5c85323b"
      },
      "source": [
        "#creating the array of labels in serial with their respective texts\n",
        "classes = ['neutral' , 'negative' , 'positive']\n",
        "y = df[classes].values\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [0, 1, 0],\n",
              "       [0, 1, 0],\n",
              "       ...,\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0]], dtype=uint8)"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1IlZCyNM8CI",
        "outputId": "a30b1e24-9d5c-42b8-cdea-7da6e9d23753"
      },
      "source": [
        "#checking for null values in train and test data\n",
        "train.isnull().any()\n",
        "test.isnull().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tweet_id      False\n",
              "tweet_text    False\n",
              "dtype: bool"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDYRa7fSM8CN"
      },
      "source": [
        "#configuration  parameters\n",
        "LATENT_DIM_DECODER = 400\n",
        "BATCH_SIZE =128\n",
        "EPOCHS = 20\n",
        "LATENT_DIM = 400\n",
        "NUM_SAMPLES = 10000\n",
        "MAX_SEQUENCE_LEN = 1000\n",
        "MAX_NUM_WORDS = 100000\n",
        "EMBEDDING_DIM = 300\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHIW6KBOM8CQ"
      },
      "source": [
        "#NLTK python library for preprocessing\n",
        "import nltk\n",
        "#nltk.download('wordnet')\n",
        "#for tokenization\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#for stemming\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "#for removing stopwords\n",
        "from nltk.corpus import stopwords\n",
        "#importing regex library of python\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n",
        "#function for performing all preproccing steps at once\n",
        "def preprocess(sentence):\n",
        "    sentence=str(sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence=sentence.replace('{html}',\"\") \n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', sentence)\n",
        "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(rem_num)  \n",
        "    filtered_words = [w for w in tokens]#  if not w in stopwords.words('english')]\n",
        "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "#make a dataframe of preprocessed text\n",
        "df['cleanText']=train_data.map(lambda s:preprocess(s)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG1pOLfvM8CU",
        "outputId": "0567fec9-38e8-4b2a-a00c-043942a63504"
      },
      "source": [
        "test['clean_text']=test['tweet_text'].map(lambda s:preprocess(s))\n",
        "test_final = test['clean_text']\n",
        "test_final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       jjuueellzz down in the atlantic city ventnor m...\n",
              "1       musical awareness great big beautiful tomorrow...\n",
              "2       on radio fm fri oct labour analyst shawn hatti...\n",
              "3       kapan sih lo ngebuktiin jan ngomong doang susa...\n",
              "4       excuse the connectivity of this live stream fr...\n",
              "                              ...                        \n",
              "5393    it s a wednesday girls night out as s band wil...\n",
              "5394    night college course sorted just have to enrol...\n",
              "5395    for the st time in years for your splendiferou...\n",
              "5396    nurses day may nursing the heart beat of the h...\n",
              "5397    we have minutes left until the nd episode of s...\n",
              "Name: clean_text, Length: 5398, dtype: object"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbbmSSHfM8CX"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGhZpHLhM8Cc"
      },
      "source": [
        "#breaking the sentence into unique words/tokens\n",
        "#expecting max tokens to be 20k\n",
        "train_final = df['cleanText']\n",
        "max_feat=40000\n",
        "#tokenize sentence into list of words\n",
        "tokenizer = Tokenizer(num_words=max_feat)#setting up tokenizer\n",
        "#fiiting the tokenizer on out data\n",
        "tokenizer.fit_on_texts(list(train_final))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMK5ShABM8Cg",
        "outputId": "21ed6094-ccbb-42d7-cc94-9535ed4b699a"
      },
      "source": [
        "train_final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        gas by my house hit i um going to chapel hill ...\n",
              "1        theo walcott is still shit uc watch rafa and j...\n",
              "2        its not that i um a gsp fan uc i just hate nic...\n",
              "3        iranian general says israel us iron dome can u...\n",
              "4        tehran uc mon amour obama tried to establish t...\n",
              "                               ...                        \n",
              "21460    the day after newark ill be able to say i met ...\n",
              "21461    fec hold farewell session for seven ministers ...\n",
              "21462    luca di montezemolo who s last day was monday ...\n",
              "21463    coffee is pretty much the answer to all questi...\n",
              "21464    niki lauda just confirmed to sky that alonso w...\n",
              "Name: cleanText, Length: 21465, dtype: object"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQI0_RfYM8Cj"
      },
      "source": [
        "tokenizer2 = Tokenizer(num_words=max_feat)#setting up tokenizer\n",
        "#fiiting the tokenizer on out data\n",
        "tokenizer2.fit_on_texts(list(test_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGVUdxbKM8Cl",
        "outputId": "19776d65-dc20-4b98-8807-a164ef4198ec"
      },
      "source": [
        "#converting text into sequence of numbers to feed in neural network\n",
        "sequence_train = tokenizer.texts_to_sequences(train_final)\n",
        "sequence_test = tokenizer2.texts_to_sequences(test_final)\n",
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 34302 unique input tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYIUmCD1M8Cn"
      },
      "source": [
        "#LOADING PRETRAINED WORD VECTORS\n",
        "# store all the pre-trained word vectors\n",
        "print('Loading word vectors...')\n",
        "word2vec = {}\n",
        "with open('/kaggle/glove.6B.300d.txt', encoding=\"utf8\") as f:\n",
        "    # is just a space-separated text file in the format:\n",
        "    # word vec[0] vec[1] vec[2] ...\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vec = np.asarray(values[1:], dtype='float32')\n",
        "word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoCcqxBM8Cr",
        "outputId": "fec0b069-dba6-408e-c583-7e013187183f"
      },
      "source": [
        "#EMBEDDING MATRIX\n",
        "# prepare embedding matrix of words for embedding layer\n",
        "print('Filling pre-trained embeddings...')\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if(i < MAX_NUM_WORDS):\n",
        "        embedding_vector = word2vec.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filling pre-trained embeddings...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh3W_nrRM8Ct",
        "outputId": "ccb4dc19-3f42-4fdc-c3ca-9bba1cba2fdc"
      },
      "source": [
        "max_len = [len(s) for s in sequence_train]\n",
        "print(max(max_len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6GnBQD6M8Cx"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_mZhXY7M8C2"
      },
      "source": [
        "#scaling all the sequences to a fixed length\n",
        "#dimension of input to the layer should be constant\n",
        "#scaling each comment sequence to a fixed length to 200\n",
        "#comments smaller than 200 will be padded with zeros to make their length as 200\n",
        "max_len=1000\n",
        "#pad the train and text sequence to be of fixed length (in keras input in lstm should be of fixed length sequnece)\n",
        "x_train=pad_sequences(sequence_train,maxlen=max_len)\n",
        "x_test=pad_sequences(sequence_test,maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z_Pt8hDM8C4"
      },
      "source": [
        "from keras.layers import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2U0vw-7M8C7"
      },
      "source": [
        "# create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "  num_words,\n",
        "  EMBEDDING_DIM,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=max_len,\n",
        "  trainable=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQK8Xg-DM8C9"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiKflcS9M8C_",
        "outputId": "239b52ed-7714-475f-ef97-065b920ac1cc"
      },
      "source": [
        "len_words = [len(words) for words in sequence_train]\n",
        "#distribution of sequence\n",
        "plt.hist(len_words, bins = np.arange(0,400,10))\n",
        "plt.show()\n",
        "# we can see that most of the comments have [0,50]  words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY9cmU6YM8DC"
      },
      "source": [
        "from keras.layers import Input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXk9DMVoM8DE"
      },
      "source": [
        "input = Input(shape=(max_len,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLcAvFyQM8DH"
      },
      "source": [
        "#feeding the output of previous layer to the embedding layer that converts \n",
        "#the sequences into vector representation to detect relevance and context \n",
        "#of a particular word\n",
        "embed_layer =embedding_layer(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2rJCOBJM8DK"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkBW5OB7M8DO"
      },
      "source": [
        "from keras.layers.recurrent import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFX9bMX6M8DQ"
      },
      "source": [
        "#passing the previous output as input to the BI_LSTM layer\n",
        "LSTM_layer = tf.keras.layers.Bidirectional(LSTM(256, return_sequences=True, name='BI_lstm_layer'))(embed_layer)\n",
        "sec_LSTM_layer = tf.keras.layers.Bidirectional(LSTM(256, return_sequences=True, name='BI2_lstm_layer'))(LSTM_layer)\n",
        "batchnorm = BatchNormalization()(sec_LSTM_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oox8fTb5M8DS"
      },
      "source": [
        "#dimension reduction using pooling layer\n",
        "red_dim_layer = tf.keras.layers.GlobalAvgPool1D()(batchnorm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o69hbR1dM8DU"
      },
      "source": [
        "##### adding dropout layer for better generalization\n",
        "#setting value as 0.1 , which means 10$ of nodes will be randomly disabled\n",
        "drop_layer = Dropout(0.55)(red_dim_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4HKUAwoM8DY"
      },
      "source": [
        "#densely connected layer\n",
        "dense1 = Dense(128,activation='elu')(drop_layer)\n",
        "batchnorm2 = BatchNormalization()(dense1)\n",
        "dense2 = Dense(128,activation='elu')(batchnorm2)\n",
        "batchnorm3 = BatchNormalization()(dense2)\n",
        "dense3 = Dense(128,activation='elu')(batchnorm3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwWpHV8vM8DZ"
      },
      "source": [
        "#adding another dropout layer\n",
        "drop_layer2 = Dropout(0.55)(dense3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7uMAK7tM8Dc"
      },
      "source": [
        "#adding the output dense layer with sigmoid activation to get result \n",
        "#3  classes as output\n",
        "output_dense = Dense(3,activation='softmax')(drop_layer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ziZ8QY5M8De",
        "outputId": "b70858af-0d4c-4a3d-f55b-4ef127f4125f"
      },
      "source": [
        "#connecting the inputs and outputs to create a model and compiling the model\n",
        "from keras.optimizers import Adagrad,Adam,RMSprop\n",
        "model = Model(inputs=input , outputs = output_dense)\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "             optimizer = RMSprop(lr=0.001),\n",
        "             metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1000)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 1000, 300)         10290900  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 1000, 512)         1140736   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 1000, 512)         1574912   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1000, 512)         2048      \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 13,108,695\n",
            "Trainable params: 13,107,159\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcRjqcHmM8Dg",
        "outputId": "1403fa88-34b5-4465-f9eb-c35d19b5adc0"
      },
      "source": [
        "#Fitting the model \n",
        "batch_size=64\n",
        "epochs = 30\n",
        "model.fit(x_train,y,batch_size=batch_size,epochs = epochs,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "269/269 [==============================] - 164s 609ms/step - loss: 1.0977 - accuracy: 0.4971 - val_loss: 1.9855 - val_accuracy: 0.3988\n",
            "Epoch 2/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.7904 - accuracy: 0.6585 - val_loss: 8.8136 - val_accuracy: 0.4281\n",
            "Epoch 3/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.6312 - accuracy: 0.7466 - val_loss: 28.4012 - val_accuracy: 0.3988\n",
            "Epoch 4/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.5122 - accuracy: 0.8026 - val_loss: 32.2878 - val_accuracy: 0.4281\n",
            "Epoch 5/30\n",
            "269/269 [==============================] - 162s 603ms/step - loss: 0.4010 - accuracy: 0.8530 - val_loss: 22.5790 - val_accuracy: 0.3990\n",
            "Epoch 6/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.3176 - accuracy: 0.8894 - val_loss: 36.6653 - val_accuracy: 0.1731\n",
            "Epoch 7/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.2551 - accuracy: 0.9094 - val_loss: 16.2503 - val_accuracy: 0.1731\n",
            "Epoch 8/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.2035 - accuracy: 0.9302 - val_loss: 9.0825 - val_accuracy: 0.4281\n",
            "Epoch 9/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.1675 - accuracy: 0.9442 - val_loss: 8.4932 - val_accuracy: 0.4298\n",
            "Epoch 10/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.1493 - accuracy: 0.9499 - val_loss: 7.6688 - val_accuracy: 0.4030\n",
            "Epoch 11/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.1296 - accuracy: 0.9580 - val_loss: 3.0278 - val_accuracy: 0.2786\n",
            "Epoch 12/30\n",
            "269/269 [==============================] - 162s 600ms/step - loss: 0.1127 - accuracy: 0.9630 - val_loss: 3.5251 - val_accuracy: 0.3084\n",
            "Epoch 13/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.1118 - accuracy: 0.9636 - val_loss: 3.1608 - val_accuracy: 0.4642\n",
            "Epoch 14/30\n",
            "269/269 [==============================] - 162s 603ms/step - loss: 0.0992 - accuracy: 0.9686 - val_loss: 1.9886 - val_accuracy: 0.5153\n",
            "Epoch 15/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.0879 - accuracy: 0.9723 - val_loss: 3.5870 - val_accuracy: 0.3450\n",
            "Epoch 16/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0818 - accuracy: 0.9746 - val_loss: 7.3300 - val_accuracy: 0.4223\n",
            "Epoch 17/30\n",
            "269/269 [==============================] - 162s 602ms/step - loss: 0.0752 - accuracy: 0.9773 - val_loss: 15.3390 - val_accuracy: 0.1731\n",
            "Epoch 18/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0668 - accuracy: 0.9790 - val_loss: 2.5116 - val_accuracy: 0.5236\n",
            "Epoch 19/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0665 - accuracy: 0.9790 - val_loss: 7.4534 - val_accuracy: 0.5022\n",
            "Epoch 20/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0641 - accuracy: 0.9797 - val_loss: 5.5781 - val_accuracy: 0.4985\n",
            "Epoch 21/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0575 - accuracy: 0.9826 - val_loss: 4.2691 - val_accuracy: 0.3487\n",
            "Epoch 23/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0547 - accuracy: 0.9842 - val_loss: 3.4779 - val_accuracy: 0.4540\n",
            "Epoch 24/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0531 - accuracy: 0.9843 - val_loss: 3.9039 - val_accuracy: 0.4642\n",
            "Epoch 25/30\n",
            "269/269 [==============================] - 161s 600ms/step - loss: 0.0478 - accuracy: 0.9863 - val_loss: 3.6402 - val_accuracy: 0.4999\n",
            "Epoch 26/30\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0530 - accuracy: 0.9841 - val_loss: 3.6919 - val_accuracy: 0.4854\n",
            "Epoch 27/30\n",
            "269/269 [==============================] - 161s 599ms/step - loss: 0.0460 - accuracy: 0.9871 - val_loss: 2.6603 - val_accuracy: 0.5157\n",
            "Epoch 28/30\n",
            "269/269 [==============================] - 161s 600ms/step - loss: 0.0465 - accuracy: 0.9870 - val_loss: 3.4747 - val_accuracy: 0.5073\n",
            "Epoch 29/30\n",
            "269/269 [==============================] - 161s 600ms/step - loss: 0.0416 - accuracy: 0.9883 - val_loss: 4.9956 - val_accuracy: 0.5183\n",
            "Epoch 30/30\n",
            "269/269 [==============================] - 161s 599ms/step - loss: 0.0426 - accuracy: 0.9877 - val_loss: 9.7257 - val_accuracy: 0.1938\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f27a8051690>"
            ]
          },
          "execution_count": 41,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    }
  ]
}